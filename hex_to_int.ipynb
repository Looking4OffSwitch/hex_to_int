{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a DNN to learn the mapping between a hex string and a digit string\n",
    "\n",
    "The goal is to use a Keras Sequential model to learn the mapping between a hex string (e.g. \"dbb9f\") and digit string (e.g. \"899999\"). The digit string represents the base numerical 10 value of the hex string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:08.587867Z",
     "start_time": "2017-08-30T21:16:05.537099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.activations import selu\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Methods for One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:08.630504Z",
     "start_time": "2017-08-30T21:16:08.589567Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = \"0123456789\"\n",
    "hex_digits = digits + \"abcdef\"\n",
    "\n",
    "digits_char_to_int = dict((c, i) for i, c in enumerate(digits))\n",
    "digits_int_to_char = dict((i, c) for i, c in enumerate(digits))\n",
    "\n",
    "hex_char_to_int = dict((c, i) for i, c in enumerate(hex_digits))\n",
    "hex_int_to_char = dict((i, c) for i, c in enumerate(hex_digits))\n",
    "\n",
    "# Encoding\n",
    "\n",
    "def one_hot_encode_hex_string(str):\n",
    "    int_encoded = [hex_char_to_int[c] for c in str]\n",
    "    return keras.utils.to_categorical(int_encoded, num_classes=len(hex_digits))\n",
    "\n",
    "def one_hot_encode_digits_string(str):\n",
    "    int_encoded = [digits_char_to_int[c] for c in str]\n",
    "    return keras.utils.to_categorical(int_encoded, num_classes=len(digits))\n",
    "\n",
    "# Decoding\n",
    "\n",
    "def one_hot_decode_hex_to_str(arr):\n",
    "    s = \"\"\n",
    "    for row in arr:\n",
    "        s += hex_int_to_char[np.argmax(row)]\n",
    "    return s   \n",
    "\n",
    "def one_hot_decode_digits_to_str(arr):\n",
    "    s = \"\"\n",
    "    for row in arr:\n",
    "        s += digits_int_to_char[np.argmax(row)]\n",
    "    return s   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T19:27:20.958431Z",
     "start_time": "2017-08-30T19:27:20.933929Z"
    }
   },
   "source": [
    "## Example hex string\n",
    "#### Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:08.640953Z",
     "start_time": "2017-08-30T21:16:08.632589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "(5, 16)\n",
      "186a2\n"
     ]
    }
   ],
   "source": [
    "# A input/hex string is *always* 5 characters\n",
    "# Range: \"186a0\" - \"dbb9f\"\n",
    "\n",
    "hex_str = \"186a2\"\n",
    "\n",
    "one_hot_encoded = one_hot_encode_hex_string(hex_str)\n",
    "print(one_hot_encoded)\n",
    "print(one_hot_encoded.shape)\n",
    "original_string = one_hot_decode_hex_to_str(one_hot_encoded)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example digit string\n",
    "#### Training label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:08.649999Z",
     "start_time": "2017-08-30T21:16:08.642860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "(6, 10)\n",
      "100002\n"
     ]
    }
   ],
   "source": [
    "# A label/digit string is *always* 6 characters\n",
    "# Range: \"100000\" - \"899999\"\n",
    "\n",
    "digit_str = \"100002\"\n",
    "\n",
    "one_hot_encoded = one_hot_encode_digits_string(digit_str)\n",
    "print(one_hot_encoded)\n",
    "print(one_hot_encoded.shape)\n",
    "original_string = one_hot_decode_digits_to_str(one_hot_encoded)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:09.519148Z",
     "start_time": "2017-08-30T21:16:08.651524Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_csv(fname, start_num, count):\n",
    "    \"\"\"\n",
    "    Create a csv named fname containing n count.\n",
    "    Each row will contain:\n",
    "        * number (starting at start_num)\n",
    "            * incremented by 1 each row\n",
    "        * the hex value of the string version of number\n",
    "    \"\"\"\n",
    "    with open(fname, \"w+\") as f:\n",
    "        result_str = \"\"\n",
    "\n",
    "        end_num = start_num + count\n",
    "        i = start_num\n",
    "\n",
    "        while i < end_num:\n",
    "            s = str(i)\n",
    "            hex_str = hex(i)[2:] # drop the leading \"0x\"\n",
    "\n",
    "            result_str += \"{},{}\\n\".format(s, hex_str)\n",
    "            i += 1\n",
    "\n",
    "        f.write(\"string,hex\\n\")\n",
    "        f.write(result_str)\n",
    "        \n",
    "data_filename = 'data.csv'        \n",
    "\n",
    "# Create it if it doesn't already exist\n",
    "if not os.path.isfile(data_filename):\n",
    "    # from 100_000 (0x186a0) to 899_999 (0xdbb9f)\n",
    "    create_csv(data_filename, 100000, 800000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.130403Z",
     "start_time": "2017-08-30T21:16:09.521039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>hex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560510</td>\n",
       "      <td>88d7e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>516793</td>\n",
       "      <td>7e2b9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>753922</td>\n",
       "      <td>b8102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136933</td>\n",
       "      <td>216e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>325765</td>\n",
       "      <td>4f885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   string    hex\n",
       "0  560510  88d7e\n",
       "1  516793  7e2b9\n",
       "2  753922  b8102\n",
       "3  136933  216e5\n",
       "4  325765  4f885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_filename, dtype={'string': str, 'hex': str})\n",
    "\n",
    "# Used to test with less data\n",
    "use_full_dataset = True\n",
    "\n",
    "if not use_full_dataset:\n",
    "    df = df[0:5]\n",
    "\n",
    "# randomize rows when using the full dataset\n",
    "if use_full_dataset:\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.set_index('string')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T19:47:33.088284Z",
     "start_time": "2017-08-30T19:47:33.086195Z"
    }
   },
   "source": [
    "## One hot encode all hex and label strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.144767Z",
     "start_time": "2017-08-30T21:16:10.131959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 6, 10)\n",
      "(800000, 5, 16)\n"
     ]
    }
   ],
   "source": [
    "all_labels_encoded = []\n",
    "all_hexes_encoded   = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    label_string = row[1] # row['string']\n",
    "    hex_str      = row[2] # row['hex']\n",
    "\n",
    "    label_encoded = one_hot_encode_digits_string(label_string)\n",
    "    hex_encoded   = one_hot_encode_hex_string(hex_str)\n",
    "\n",
    "    all_labels_encoded.append(label_encoded)\n",
    "    all_hexes_encoded.append(hex_encoded)\n",
    "\n",
    "all_labels_encoded = np.asarray(all_labels_encoded)\n",
    "all_hexes_encoded  = np.asarray(all_hexes_encoded)\n",
    "\n",
    "print(all_labels_encoded.shape)\n",
    "print(all_hexes_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T19:48:23.316727Z",
     "start_time": "2017-08-30T19:48:23.314334Z"
    }
   },
   "source": [
    "## Verify that encoding decoding works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.151748Z",
     "start_time": "2017-08-30T21:16:10.146299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560510 -> 88d7e\n",
      "516793 -> 7e2b9\n",
      "753922 -> b8102\n",
      "136933 -> 216e5\n",
      "325765 -> 4f885\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(one_hot_decode_digits_to_str(all_labels_encoded[i]), \n",
    "          \"->\", \n",
    "          one_hot_decode_hex_to_str(all_hexes_encoded[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split encoded arrays into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.178956Z",
     "start_time": "2017-08-30T21:16:10.152988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expected  : 800000\n",
      "Total calculated: 800000\n",
      "Training examples: 560000\n",
      "Validation examples: 120000\n",
      "Test examples 120000\n"
     ]
    }
   ],
   "source": [
    "if use_full_dataset:\n",
    "    example_cnt = len(df)\n",
    "    n_training_examples   = int(example_cnt * 0.7)\n",
    "    n_validation_examples = int(example_cnt * 0.15)\n",
    "    n_test_examples       = int(example_cnt * 0.15)\n",
    "else:\n",
    "    n_training_examples   = 3\n",
    "    n_validation_examples = 1\n",
    "    n_test_examples       = 1\n",
    "\n",
    "print(\"Total expected  :\", len(df))\n",
    "print(\"Total calculated:\", n_training_examples + n_validation_examples + n_test_examples)\n",
    "\n",
    "x_train = all_hexes_encoded[:n_training_examples]\n",
    "y_train = all_labels_encoded[:n_training_examples]\n",
    "print(\"Training examples:\", len(x_train))\n",
    "\n",
    "x_validation = all_hexes_encoded[n_training_examples  : n_training_examples + n_validation_examples]\n",
    "y_validation = all_labels_encoded[n_training_examples : n_training_examples + n_validation_examples]\n",
    "print(\"Validation examples:\", len(x_validation))\n",
    "\n",
    "x_test = all_hexes_encoded[len(all_hexes_encoded) - n_test_examples:]\n",
    "y_test = all_labels_encoded[len(all_labels_encoded) - n_test_examples:]\n",
    "print(\"Test examples\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of training and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.185869Z",
     "start_time": "2017-08-30T21:16:10.180818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (560000, 5, 16)\n",
      "y_train.shape: (560000, 6, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape:\", x_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify our dimensions match our expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.201200Z",
     "start_time": "2017-08-30T21:16:10.187691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dimensions match: True\n",
      "Label dimensions match: True\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Dimensions of the input matrix: one hot encoded hex string (.e.g \"186a2\")\n",
    "#\n",
    "\n",
    "# fixed length (in chars) of a hex string\n",
    "hex_n = len(one_hot_decode_hex_to_str(all_hexes_encoded[0]))\n",
    "\n",
    "# number of symbols in the \"alphabet\" of a hex string\n",
    "hex_k = len(hex_digits)\n",
    "\n",
    "print(\"Training dimensions match:\", x_train.shape[1:3] == (hex_n, hex_k))\n",
    "\n",
    "#\n",
    "# Dimensions of the label matrix: one hot encoded digits string (.e.g \"100002\")\n",
    "#\n",
    "\n",
    "# fixed length (in chars) of a hex string\n",
    "label_n = len(one_hot_decode_digits_to_str(all_labels_encoded[0]))\n",
    "\n",
    "# number of symbols in the \"alphabet\" of a digits string\n",
    "label_k = len(digits)\n",
    "\n",
    "print(\"Label dimensions match:\", y_train.shape[1:3] == (label_n, label_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and compile model\n",
    "\n",
    "### Questions\n",
    "* How do we determine the correct number of nodes in the initial layer?\n",
    "* The shape of the input data is `(5, 16)` and the shape of our labels is `(6, 10)`. How do we define a model that can convert between these differing dimensions?\n",
    "* Do we have to create our own [loss function implementation](https://stackoverflow.com/questions/43576922/keras-custom-metric-iteration) to make this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.222200Z",
     "start_time": "2017-08-30T21:16:10.203035Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 5 #input size\n",
    "m = 6 #output size\n",
    "l = 16 #input alphabet size\n",
    "k = 10 #output alpahbet size\n",
    "bs = 512 #batch size\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (n,l)))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(128, activation = selu))\n",
    "model.add(Dense(m * k, activation=None))\n",
    "model.add(Reshape((m, k)))\n",
    "\n",
    "def batch_accuracy(truth, pred):\n",
    "    truth_decoded = tf.argmax(truth, axis = 2)\n",
    "    pred_decoded = tf.argmax(pred, axis = 2)\n",
    "    correct = tf.cast(tf.equal(truth_decoded, pred_decoded), tf.float32)\n",
    "    #correct is bs x m\n",
    "    accuracy = tf.reduce_sum(correct, axis = 1)/float(m)\n",
    "    return accuracy\n",
    "\n",
    "def batch_crossentropy(truth, pred):\n",
    "    batch_size = tf.shape(truth)[0]\n",
    "    #input shapes are BxMxK where B is batchsize\n",
    "    #reshape so that every row is a separate probability distribution\n",
    "    truth = tf.reshape(truth, (-1, k))\n",
    "    pred = tf.reshape(pred, (-1, k))\n",
    "    ce = tf.nn.softmax_cross_entropy_with_logits(labels = truth, logits = pred)\n",
    "    #crossentropy is now bs * m, and we want it reshaped to bs x m\n",
    "    ce = tf.reshape(ce, (batch_size,-1))\n",
    "    loss = tf.reduce_sum(ce, axis = 1)\n",
    "    return loss\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss=batch_crossentropy,\n",
    "              metrics = [batch_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.227412Z",
     "start_time": "2017-08-30T21:16:10.223969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560000 samples, validate on 120000 samples\n",
      "Epoch 1/100\n",
      "560000/560000 [==============================] - 5s - loss: 7.8639 - batch_accuracy: 0.4738 - val_loss: 6.7099 - val_batch_accuracy: 0.5430\n",
      "Epoch 2/100\n",
      "560000/560000 [==============================] - 5s - loss: 5.9899 - batch_accuracy: 0.5883 - val_loss: 5.1754 - val_batch_accuracy: 0.6331\n",
      "Epoch 3/100\n",
      "560000/560000 [==============================] - 6s - loss: 4.7082 - batch_accuracy: 0.6667 - val_loss: 4.5591 - val_batch_accuracy: 0.6772\n",
      "Epoch 4/100\n",
      "560000/560000 [==============================] - 6s - loss: 4.4910 - batch_accuracy: 0.6804 - val_loss: 4.6240 - val_batch_accuracy: 0.6699\n",
      "Epoch 5/100\n",
      "560000/560000 [==============================] - 6s - loss: 4.3986 - batch_accuracy: 0.6870 - val_loss: 4.3614 - val_batch_accuracy: 0.6911\n",
      "Epoch 6/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.3423 - batch_accuracy: 0.6907 - val_loss: 4.2879 - val_batch_accuracy: 0.6948\n",
      "Epoch 7/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.3012 - batch_accuracy: 0.6935 - val_loss: 4.2546 - val_batch_accuracy: 0.6958\n",
      "Epoch 8/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.2741 - batch_accuracy: 0.6952 - val_loss: 4.2458 - val_batch_accuracy: 0.6969\n",
      "Epoch 9/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.2508 - batch_accuracy: 0.6967 - val_loss: 4.2281 - val_batch_accuracy: 0.6975\n",
      "Epoch 10/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.2298 - batch_accuracy: 0.6984 - val_loss: 4.1847 - val_batch_accuracy: 0.7011\n",
      "Epoch 11/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.2127 - batch_accuracy: 0.6996 - val_loss: 4.2651 - val_batch_accuracy: 0.6959\n",
      "Epoch 12/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.1967 - batch_accuracy: 0.7005 - val_loss: 4.1708 - val_batch_accuracy: 0.7019\n",
      "Epoch 13/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.1807 - batch_accuracy: 0.7019 - val_loss: 4.1651 - val_batch_accuracy: 0.7030\n",
      "Epoch 14/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.1669 - batch_accuracy: 0.7029 - val_loss: 4.1928 - val_batch_accuracy: 0.7000\n",
      "Epoch 15/100\n",
      "560000/560000 [==============================] - 6s - loss: 4.1517 - batch_accuracy: 0.7044 - val_loss: 4.1417 - val_batch_accuracy: 0.7049\n",
      "Epoch 16/100\n",
      "560000/560000 [==============================] - 6s - loss: 4.1369 - batch_accuracy: 0.7057 - val_loss: 4.2478 - val_batch_accuracy: 0.6992\n",
      "Epoch 17/100\n",
      "560000/560000 [==============================] - 5s - loss: 4.1056 - batch_accuracy: 0.7088 - val_loss: 4.0729 - val_batch_accuracy: 0.7130\n",
      "Epoch 18/100\n",
      "560000/560000 [==============================] - 5s - loss: 3.4098 - batch_accuracy: 0.7492 - val_loss: 2.7816 - val_batch_accuracy: 0.7843\n",
      "Epoch 19/100\n",
      "560000/560000 [==============================] - 5s - loss: 2.4927 - batch_accuracy: 0.8064 - val_loss: 2.4573 - val_batch_accuracy: 0.8069\n",
      "Epoch 20/100\n",
      "560000/560000 [==============================] - 5s - loss: 2.3021 - batch_accuracy: 0.8194 - val_loss: 2.2685 - val_batch_accuracy: 0.8223\n",
      "Epoch 21/100\n",
      "560000/560000 [==============================] - 5s - loss: 2.2047 - batch_accuracy: 0.8270 - val_loss: 2.3613 - val_batch_accuracy: 0.8148\n",
      "Epoch 22/100\n",
      "560000/560000 [==============================] - 5s - loss: 2.1319 - batch_accuracy: 0.8330 - val_loss: 2.0915 - val_batch_accuracy: 0.8340\n",
      "Epoch 23/100\n",
      "560000/560000 [==============================] - 5s - loss: 2.0726 - batch_accuracy: 0.8389 - val_loss: 2.0507 - val_batch_accuracy: 0.8408\n",
      "Epoch 24/100\n",
      "560000/560000 [==============================] - 6s - loss: 2.0033 - batch_accuracy: 0.8470 - val_loss: 2.2988 - val_batch_accuracy: 0.8300\n",
      "Epoch 25/100\n",
      "560000/560000 [==============================] - 6s - loss: 1.8867 - batch_accuracy: 0.8608 - val_loss: 1.5635 - val_batch_accuracy: 0.8903\n",
      "Epoch 26/100\n",
      "560000/560000 [==============================] - 6s - loss: 1.6650 - batch_accuracy: 0.8843 - val_loss: 1.4142 - val_batch_accuracy: 0.9042\n",
      "Epoch 27/100\n",
      "560000/560000 [==============================] - 6s - loss: 1.3518 - batch_accuracy: 0.9117 - val_loss: 0.9378 - val_batch_accuracy: 0.9411\n",
      "Epoch 28/100\n",
      "560000/560000 [==============================] - 5s - loss: 1.1297 - batch_accuracy: 0.9289 - val_loss: 0.8031 - val_batch_accuracy: 0.9541\n",
      "Epoch 29/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.9938 - batch_accuracy: 0.9393 - val_loss: 1.9619 - val_batch_accuracy: 0.8749\n",
      "Epoch 30/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.8837 - batch_accuracy: 0.9481 - val_loss: 1.0387 - val_batch_accuracy: 0.9442\n",
      "Epoch 31/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.8062 - batch_accuracy: 0.9535 - val_loss: 2.1437 - val_batch_accuracy: 0.8659\n",
      "Epoch 32/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.7454 - batch_accuracy: 0.9583 - val_loss: 0.3883 - val_batch_accuracy: 0.9799\n",
      "Epoch 33/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.6961 - batch_accuracy: 0.9617 - val_loss: 0.2956 - val_batch_accuracy: 0.9862\n",
      "Epoch 34/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.6548 - batch_accuracy: 0.9644 - val_loss: 0.2735 - val_batch_accuracy: 0.9864\n",
      "Epoch 35/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.6211 - batch_accuracy: 0.9667 - val_loss: 1.6068 - val_batch_accuracy: 0.8986\n",
      "Epoch 36/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.5935 - batch_accuracy: 0.9684 - val_loss: 0.3214 - val_batch_accuracy: 0.9833\n",
      "Epoch 37/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.5672 - batch_accuracy: 0.9700 - val_loss: 0.2322 - val_batch_accuracy: 0.9888\n",
      "Epoch 38/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.5448 - batch_accuracy: 0.9715 - val_loss: 2.6910 - val_batch_accuracy: 0.8635\n",
      "Epoch 39/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.5224 - batch_accuracy: 0.9729 - val_loss: 0.2762 - val_batch_accuracy: 0.9850\n",
      "Epoch 40/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.5045 - batch_accuracy: 0.9737 - val_loss: 0.2253 - val_batch_accuracy: 0.9884\n",
      "Epoch 41/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4924 - batch_accuracy: 0.9746 - val_loss: 0.1763 - val_batch_accuracy: 0.9916\n",
      "Epoch 42/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4740 - batch_accuracy: 0.9759 - val_loss: 0.1870 - val_batch_accuracy: 0.9905\n",
      "Epoch 43/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4570 - batch_accuracy: 0.9770 - val_loss: 0.1511 - val_batch_accuracy: 0.9932\n",
      "Epoch 44/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4446 - batch_accuracy: 0.9776 - val_loss: 0.1833 - val_batch_accuracy: 0.9917\n",
      "Epoch 45/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4258 - batch_accuracy: 0.9785 - val_loss: 0.1764 - val_batch_accuracy: 0.9912\n",
      "Epoch 46/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4189 - batch_accuracy: 0.9792 - val_loss: 2.7318 - val_batch_accuracy: 0.8694\n",
      "Epoch 47/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.4084 - batch_accuracy: 0.9798 - val_loss: 0.1336 - val_batch_accuracy: 0.9938\n",
      "Epoch 48/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3934 - batch_accuracy: 0.9807 - val_loss: 0.6618 - val_batch_accuracy: 0.9629\n",
      "Epoch 49/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3909 - batch_accuracy: 0.9805 - val_loss: 0.1339 - val_batch_accuracy: 0.9938\n",
      "Epoch 50/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3793 - batch_accuracy: 0.9813 - val_loss: 0.1465 - val_batch_accuracy: 0.9930\n",
      "Epoch 51/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.3640 - batch_accuracy: 0.9820 - val_loss: 2.2728 - val_batch_accuracy: 0.8925\n",
      "Epoch 52/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3599 - batch_accuracy: 0.9822 - val_loss: 0.1024 - val_batch_accuracy: 0.9961\n",
      "Epoch 53/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3524 - batch_accuracy: 0.9826 - val_loss: 0.1230 - val_batch_accuracy: 0.9950\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560000/560000 [==============================] - 5s - loss: 0.3486 - batch_accuracy: 0.9829 - val_loss: 0.1011 - val_batch_accuracy: 0.9957\n",
      "Epoch 55/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3359 - batch_accuracy: 0.9837 - val_loss: 0.1504 - val_batch_accuracy: 0.9930\n",
      "Epoch 56/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3348 - batch_accuracy: 0.9837 - val_loss: 0.0771 - val_batch_accuracy: 0.9973\n",
      "Epoch 57/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3201 - batch_accuracy: 0.9843 - val_loss: 0.3165 - val_batch_accuracy: 0.9811\n",
      "Epoch 58/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3211 - batch_accuracy: 0.9842 - val_loss: 0.1683 - val_batch_accuracy: 0.9934\n",
      "Epoch 59/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3124 - batch_accuracy: 0.9848 - val_loss: 0.0817 - val_batch_accuracy: 0.9967\n",
      "Epoch 60/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3071 - batch_accuracy: 0.9852 - val_loss: 0.0737 - val_batch_accuracy: 0.9969\n",
      "Epoch 61/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.3022 - batch_accuracy: 0.9855 - val_loss: 0.2785 - val_batch_accuracy: 0.9849\n",
      "Epoch 62/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2986 - batch_accuracy: 0.9855 - val_loss: 0.1228 - val_batch_accuracy: 0.9951\n",
      "Epoch 63/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2927 - batch_accuracy: 0.9861 - val_loss: 0.4087 - val_batch_accuracy: 0.9762\n",
      "Epoch 64/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2845 - batch_accuracy: 0.9863 - val_loss: 0.1105 - val_batch_accuracy: 0.9947\n",
      "Epoch 65/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2820 - batch_accuracy: 0.9865 - val_loss: 0.0762 - val_batch_accuracy: 0.9961\n",
      "Epoch 66/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2827 - batch_accuracy: 0.9862 - val_loss: 0.1136 - val_batch_accuracy: 0.9947\n",
      "Epoch 67/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2731 - batch_accuracy: 0.9870 - val_loss: 0.4103 - val_batch_accuracy: 0.9841\n",
      "Epoch 68/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2787 - batch_accuracy: 0.9867 - val_loss: 0.0590 - val_batch_accuracy: 0.9976\n",
      "Epoch 69/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2674 - batch_accuracy: 0.9873 - val_loss: 0.0645 - val_batch_accuracy: 0.9973\n",
      "Epoch 70/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2669 - batch_accuracy: 0.9874 - val_loss: 0.0590 - val_batch_accuracy: 0.9978\n",
      "Epoch 71/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2654 - batch_accuracy: 0.9875 - val_loss: 0.1181 - val_batch_accuracy: 0.9953\n",
      "Epoch 72/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2610 - batch_accuracy: 0.9875 - val_loss: 0.0518 - val_batch_accuracy: 0.9980\n",
      "Epoch 73/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2604 - batch_accuracy: 0.9877 - val_loss: 0.0666 - val_batch_accuracy: 0.9972\n",
      "Epoch 74/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2470 - batch_accuracy: 0.9882 - val_loss: 0.5641 - val_batch_accuracy: 0.9696\n",
      "Epoch 75/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2508 - batch_accuracy: 0.9881 - val_loss: 0.0531 - val_batch_accuracy: 0.9975\n",
      "Epoch 76/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2470 - batch_accuracy: 0.9883 - val_loss: 0.1093 - val_batch_accuracy: 0.9947\n",
      "Epoch 77/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2456 - batch_accuracy: 0.9882 - val_loss: 0.0800 - val_batch_accuracy: 0.9969\n",
      "Epoch 78/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2421 - batch_accuracy: 0.9886 - val_loss: 0.0449 - val_batch_accuracy: 0.9981\n",
      "Epoch 79/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2383 - batch_accuracy: 0.9889 - val_loss: 0.0480 - val_batch_accuracy: 0.9982\n",
      "Epoch 80/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2349 - batch_accuracy: 0.9892 - val_loss: 0.2329 - val_batch_accuracy: 0.9883\n",
      "Epoch 81/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2402 - batch_accuracy: 0.9888 - val_loss: 0.0495 - val_batch_accuracy: 0.9980\n",
      "Epoch 82/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2283 - batch_accuracy: 0.9893 - val_loss: 2.3339 - val_batch_accuracy: 0.8916\n",
      "Epoch 83/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2299 - batch_accuracy: 0.9893 - val_loss: 0.0416 - val_batch_accuracy: 0.9984\n",
      "Epoch 84/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2285 - batch_accuracy: 0.9893 - val_loss: 0.1472 - val_batch_accuracy: 0.9919\n",
      "Epoch 85/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2238 - batch_accuracy: 0.9897 - val_loss: 0.0572 - val_batch_accuracy: 0.9978\n",
      "Epoch 86/100\n",
      "560000/560000 [==============================] - 6s - loss: 0.2182 - batch_accuracy: 0.9897 - val_loss: 0.0497 - val_batch_accuracy: 0.9979\n",
      "Epoch 87/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2176 - batch_accuracy: 0.9900 - val_loss: 0.0412 - val_batch_accuracy: 0.9983\n",
      "Epoch 88/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2119 - batch_accuracy: 0.9902 - val_loss: 0.1507 - val_batch_accuracy: 0.9938\n",
      "Epoch 89/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2206 - batch_accuracy: 0.9898 - val_loss: 0.0536 - val_batch_accuracy: 0.9978\n",
      "Epoch 90/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2138 - batch_accuracy: 0.9903 - val_loss: 0.0850 - val_batch_accuracy: 0.9958\n",
      "Epoch 91/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2120 - batch_accuracy: 0.9902 - val_loss: 0.1361 - val_batch_accuracy: 0.9952\n",
      "Epoch 92/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2160 - batch_accuracy: 0.9902 - val_loss: 1.9871 - val_batch_accuracy: 0.9193\n",
      "Epoch 93/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2123 - batch_accuracy: 0.9902 - val_loss: 1.7174 - val_batch_accuracy: 0.9109\n",
      "Epoch 94/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2048 - batch_accuracy: 0.9906 - val_loss: 0.1740 - val_batch_accuracy: 0.9903\n",
      "Epoch 95/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2115 - batch_accuracy: 0.9904 - val_loss: 0.0663 - val_batch_accuracy: 0.9977\n",
      "Epoch 96/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2005 - batch_accuracy: 0.9909 - val_loss: 0.1389 - val_batch_accuracy: 0.9924\n",
      "Epoch 97/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2048 - batch_accuracy: 0.9905 - val_loss: 0.0472 - val_batch_accuracy: 0.9980\n",
      "Epoch 98/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2017 - batch_accuracy: 0.9907 - val_loss: 2.7614 - val_batch_accuracy: 0.8911\n",
      "Epoch 99/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.1987 - batch_accuracy: 0.9912 - val_loss: 0.1209 - val_batch_accuracy: 0.9966\n",
      "Epoch 100/100\n",
      "560000/560000 [==============================] - 5s - loss: 0.2032 - batch_accuracy: 0.9909 - val_loss: 0.0403 - val_batch_accuracy: 0.9981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38600760f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size = bs, verbose = 1, epochs = 100, \n",
    "          validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Evaluate the trained model on data it has never seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T21:16:10.234299Z",
     "start_time": "2017-08-30T21:16:10.229590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119776/120000 [============================>.] - ETA: 0s\n",
      "loss    : 0.04203913698233664\n",
      "accuracy: 99.81083250204722\n",
      "485164 485164\n",
      "859720 859720\n",
      "384963 384963\n",
      "192507 192507\n",
      "232472 232472\n",
      "810523 810523\n",
      "524557 524557\n",
      "838359 838359\n",
      "339155 339155\n",
      "195101 195101\n",
      "841439 841439\n",
      "627070 627070\n",
      "501053 502053\n",
      "650990 650990\n",
      "809489 809489\n",
      "720114 720114\n",
      "397322 397322\n",
      "142904 142904\n",
      "104765 104765\n",
      "782915 782915\n",
      "519743 519743\n",
      "758133 758133\n",
      "378099 378099\n",
      "378695 378695\n",
      "669149 669149\n",
      "747095 747095\n",
      "358766 358766\n",
      "150366 150366\n",
      "328170 328170\n",
      "430582 430582\n",
      "725541 725541\n",
      "189279 188279\n",
      "439582 439582\n",
      "840894 840894\n",
      "723072 723072\n",
      "252074 252074\n",
      "309252 309252\n",
      "364087 364087\n",
      "798106 798106\n",
      "436341 436341\n",
      "499855 499855\n",
      "362789 362789\n",
      "580358 580358\n",
      "763875 763875\n",
      "227467 227467\n",
      "348483 348483\n",
      "652181 652181\n",
      "466108 466108\n",
      "250581 250581\n",
      "725330 725330\n",
      "405879 405879\n",
      "785607 785607\n",
      "254648 254648\n",
      "577889 577889\n",
      "540710 540710\n",
      "654654 654654\n",
      "389617 389617\n",
      "563833 563833\n",
      "390384 390384\n",
      "212799 212799\n",
      "248120 248120\n",
      "653491 653491\n",
      "290388 290388\n",
      "441727 441727\n",
      "212621 212621\n",
      "611378 611378\n",
      "887444 887444\n",
      "554444 554444\n",
      "113922 113922\n",
      "800719 800719\n",
      "274163 274163\n",
      "709386 709386\n",
      "786776 786776\n",
      "498155 498155\n",
      "638188 638188\n",
      "396712 396712\n",
      "312778 312778\n",
      "139400 139400\n",
      "759101 759101\n",
      "683722 683722\n",
      "535203 535203\n",
      "353955 353955\n",
      "293135 293135\n",
      "427645 427645\n",
      "578265 578265\n",
      "149181 149181\n",
      "530716 530716\n",
      "405931 405931\n",
      "203192 203192\n",
      "319438 319438\n",
      "371729 371729\n",
      "693377 693377\n",
      "120500 120500\n",
      "780837 780837\n",
      "125664 125664\n",
      "702202 702202\n",
      "309688 309688\n",
      "713542 713542\n",
      "156001 156001\n",
      "320703 320703\n",
      "635530 635530\n",
      "521590 521590\n",
      "168260 168260\n",
      "501921 501921\n",
      "432744 432744\n",
      "703625 703625\n",
      "277958 277958\n",
      "549585 549585\n",
      "759294 759294\n",
      "795707 795707\n",
      "392454 392454\n",
      "710872 710872\n",
      "688656 688656\n",
      "301514 301514\n",
      "618943 618943\n",
      "313466 313466\n",
      "475189 475189\n",
      "292100 292100\n",
      "250258 250258\n",
      "138062 138062\n",
      "743971 743971\n",
      "821430 821430\n",
      "367996 367996\n",
      "436736 436736\n",
      "406831 406831\n",
      "255086 255086\n",
      "887904 887904\n",
      "323988 323988\n",
      "496959 496959\n",
      "617520 617520\n",
      "108308 108308\n",
      "588663 588663\n",
      "876598 876598\n",
      "806875 806875\n",
      "174614 174614\n",
      "318741 318741\n",
      "661033 661033\n",
      "262583 262583\n",
      "340847 340847\n",
      "627422 627422\n",
      "496047 496047\n",
      "813342 813342\n",
      "483478 483478\n",
      "875927 875927\n",
      "323911 323911\n",
      "651076 651076\n",
      "772014 772014\n",
      "437620 437620\n",
      "869724 869724\n",
      "325594 325594\n",
      "541914 541914\n",
      "307261 307261\n",
      "789927 789927\n",
      "201799 201799\n",
      "499318 499318\n",
      "312049 312049\n",
      "437763 437763\n",
      "286237 286237\n",
      "535316 535316\n",
      "316948 316948\n",
      "761791 761791\n",
      "365906 365906\n",
      "857540 857540\n",
      "754866 754866\n",
      "289868 289868\n",
      "805245 805245\n",
      "758698 758698\n",
      "431639 431639\n",
      "727443 727443\n",
      "294049 294049\n",
      "462467 462467\n",
      "397450 397450\n",
      "439144 439144\n",
      "137648 137648\n",
      "760285 760285\n",
      "550607 550607\n",
      "652221 652221\n",
      "225631 225631\n",
      "615455 615455\n",
      "357579 357579\n",
      "332274 332274\n",
      "707035 707035\n",
      "113874 113874\n",
      "498075 498075\n",
      "859479 859479\n",
      "318298 318298\n",
      "346016 346016\n",
      "181109 181109\n",
      "236461 236461\n",
      "692011 692011\n",
      "180732 180732\n",
      "338888 338888\n",
      "758885 758885\n",
      "332112 332112\n",
      "768395 768395\n",
      "131489 131489\n",
      "860665 860665\n",
      "268317 268317\n",
      "285741 285741\n",
      "458159 458159\n",
      "479738 479738\n",
      "631471 631471\n",
      "774382 774382\n",
      "722441 722441\n",
      "333576 333576\n",
      "441553 441553\n",
      "786577 786577\n",
      "403600 403600\n",
      "536280 536280\n",
      "531754 531754\n",
      "644873 644873\n",
      "827436 827436\n",
      "758496 758496\n",
      "520629 520629\n",
      "617657 617657\n",
      "701662 701662\n",
      "171158 171158\n",
      "301226 301226\n",
      "681690 681690\n",
      "314649 314649\n",
      "324785 324785\n",
      "115988 115988\n",
      "726768 726768\n",
      "438324 438324\n",
      "747399 747399\n",
      "390501 390501\n",
      "844713 844713\n",
      "344767 344767\n",
      "722425 722425\n",
      "125279 125279\n",
      "785915 785915\n",
      "332621 332621\n",
      "700207 700207\n",
      "661294 661294\n",
      "547476 547476\n",
      "658900 658900\n",
      "437845 437845\n",
      "536681 536681\n",
      "517887 517887\n",
      "444949 444949\n",
      "437029 437029\n",
      "881565 881565\n",
      "761834 761834\n",
      "369964 369964\n",
      "725077 725073\n",
      "748161 748161\n",
      "211612 211612\n",
      "856554 856554\n",
      "344328 344328\n",
      "795500 795500\n",
      "581099 581099\n",
      "313492 313492\n",
      "673514 673514\n",
      "716583 716583\n",
      "479177 479177\n",
      "177505 177505\n",
      "403836 403836\n",
      "195463 195463\n",
      "893847 893847\n",
      "743908 743908\n",
      "483874 483874\n",
      "120758 120758\n",
      "132192 132198\n",
      "142542 142542\n",
      "773635 773635\n",
      "220810 220810\n",
      "764353 764353\n",
      "670913 669913\n",
      "854735 854735\n",
      "725007 725007\n",
      "777373 777373\n",
      "616453 616453\n",
      "826335 826335\n",
      "843850 843850\n",
      "820198 820198\n",
      "173865 173865\n",
      "748730 748730\n",
      "155825 155825\n",
      "558951 558951\n",
      "767691 767691\n",
      "820883 820883\n",
      "780130 780130\n",
      "678816 678816\n",
      "421680 421680\n",
      "325443 325443\n",
      "360694 360694\n",
      "272309 272309\n",
      "769674 769674\n",
      "370345 370345\n",
      "519900 519900\n",
      "720500 720500\n",
      "655256 655256\n",
      "239680 239680\n",
      "278784 278784\n",
      "748778 748778\n",
      "862643 862643\n",
      "876018 876018\n",
      "278999 278999\n",
      "673902 673902\n",
      "126534 126534\n",
      "501966 501966\n",
      "393697 393697\n",
      "168079 168079\n",
      "361341 361341\n",
      "613504 613504\n",
      "310581 319581\n",
      "223973 223973\n",
      "419857 419857\n",
      "563420 563420\n",
      "848155 848155\n",
      "277526 277526\n",
      "195988 195988\n",
      "785994 785994\n",
      "742334 742334\n",
      "297535 297535\n",
      "409419 409419\n",
      "166987 166987\n",
      "463200 463200\n",
      "881850 881850\n",
      "706943 706943\n",
      "791856 791856\n",
      "654383 654383\n",
      "783375 783375\n",
      "673192 673192\n",
      "657496 657496\n",
      "297889 297889\n",
      "877563 877563\n",
      "695861 695861\n",
      "851378 851378\n",
      "757887 757887\n",
      "545635 545635\n",
      "435570 435570\n",
      "289317 289317\n",
      "114745 114745\n",
      "248148 248148\n",
      "574024 574024\n",
      "745958 745958\n",
      "132705 132705\n",
      "810257 810257\n",
      "875257 875257\n",
      "540604 540604\n",
      "173594 173594\n",
      "228710 228710\n",
      "428359 428359\n",
      "620118 620118\n",
      "442911 442911\n",
      "141612 141612\n",
      "784798 784798\n",
      "131996 131996\n",
      "315291 315291\n",
      "613802 613802\n",
      "148926 148926\n",
      "306786 306786\n",
      "417966 417966\n",
      "610662 610662\n",
      "208359 208359\n",
      "607622 607622\n",
      "320272 320272\n",
      "365120 365120\n",
      "326019 326019\n",
      "741873 741873\n",
      "700249 700249\n",
      "180323 180323\n",
      "286761 286761\n",
      "680524 680524\n",
      "600169 600169\n",
      "896992 896992\n",
      "677940 677940\n",
      "891499 891499\n",
      "106088 106088\n",
      "109583 109583\n",
      "526272 526272\n",
      "100111 100111\n",
      "263775 263775\n",
      "517901 517901\n",
      "364185 364185\n",
      "513867 513867\n",
      "453988 453988\n",
      "564535 564535\n",
      "515414 515414\n",
      "771051 771051\n",
      "389752 389752\n",
      "296109 296109\n",
      "822858 822858\n",
      "647915 647915\n",
      "709341 709341\n",
      "872880 872880\n",
      "153753 153753\n",
      "441119 441119\n",
      "383703 383703\n",
      "521037 521037\n",
      "280301 280301\n",
      "373668 373668\n",
      "459599 459599\n",
      "531701 531701\n",
      "205713 205713\n",
      "258135 258135\n",
      "191557 191557\n",
      "228714 228714\n",
      "712140 712140\n",
      "803937 803937\n",
      "243465 243465\n",
      "209899 209899\n",
      "283569 283569\n",
      "592774 592774\n",
      "656173 656173\n",
      "391965 391965\n",
      "647514 647514\n",
      "658425 658425\n",
      "529651 529651\n",
      "682713 682713\n",
      "531404 531404\n",
      "802779 802779\n",
      "225240 225240\n",
      "645014 645014\n",
      "389018 389018\n",
      "833900 833900\n",
      "266981 266981\n",
      "566343 566343\n",
      "537268 537268\n",
      "174588 174588\n",
      "471133 471133\n",
      "686282 686282\n",
      "499902 499902\n",
      "241484 241484\n",
      "569567 569567\n",
      "673923 673923\n",
      "607067 607067\n",
      "761702 761702\n",
      "800537 800537\n",
      "492841 492841\n",
      "183791 183791\n",
      "193863 193863\n",
      "672883 672883\n",
      "131720 131720\n",
      "767497 767497\n",
      "744622 744622\n",
      "641703 641703\n",
      "756159 756159\n",
      "598044 598044\n",
      "291348 291348\n",
      "211983 211983\n",
      "781896 781896\n",
      "553874 553874\n",
      "281431 281431\n",
      "364966 364966\n",
      "873393 873393\n",
      "495712 495712\n",
      "633957 633957\n",
      "500556 500556\n",
      "780205 780205\n",
      "123829 123829\n",
      "886007 886007\n",
      "862616 862616\n",
      "242001 242001\n",
      "726914 726914\n",
      "432723 432723\n",
      "585829 585829\n",
      "577468 577468\n",
      "857419 857419\n",
      "897471 897471\n",
      "512599 512599\n",
      "792119 792119\n",
      "837487 837487\n",
      "231907 231907\n",
      "880637 880637\n",
      "535345 535345\n",
      "246766 246766\n",
      "837561 837561\n",
      "330578 330578\n",
      "835614 835614\n",
      "154275 154275\n",
      "709724 709724\n",
      "508691 508691\n",
      "464917 464917\n",
      "746873 746873\n",
      "578731 578731\n",
      "628901 628901\n",
      "132398 132398\n",
      "355394 355394\n",
      "858303 858303\n",
      "716543 716543\n",
      "176324 176324\n",
      "472503 472503\n",
      "478209 478209\n",
      "437407 437407\n",
      "524991 524991\n",
      "747475 747475\n",
      "818752 818752\n",
      "128922 128922\n",
      "782026 782026\n",
      "109950 109950\n",
      "713823 713823\n",
      "402213 402213\n",
      "363264 363264\n",
      "752451 752451\n",
      "112469 112469\n",
      "725016 725016\n",
      "279598 279598\n",
      "465586 465586\n",
      "569694 569694\n",
      "899839 899839\n",
      "234247 234247\n",
      "419327 419327\n",
      "309827 309827\n",
      "612604 612604\n",
      "194621 194621\n",
      "425325 425325\n",
      "875265 875265\n",
      "415821 415821\n",
      "108983 108983\n",
      "893009 893009\n",
      "330914 330914\n",
      "630409 630409\n",
      "381042 381042\n",
      "401794 401794\n",
      "847751 847751\n",
      "465627 465627\n",
      "641461 641461\n",
      "271906 271906\n",
      "691709 691709\n",
      "127054 127054\n",
      "407019 407019\n",
      "226073 226073\n",
      "808931 808931\n",
      "152189 152189\n",
      "161960 161960\n",
      "568299 568299\n",
      "537297 537297\n",
      "764414 764414\n",
      "517371 517371\n",
      "768545 768545\n",
      "158103 158103\n",
      "695987 695987\n",
      "770571 770571\n",
      "787350 787350\n",
      "264058 264058\n",
      "679257 679257\n",
      "820167 820167\n",
      "681071 681071\n",
      "463961 463961\n",
      "534443 534443\n",
      "104275 104275\n",
      "429121 429121\n",
      "434936 434936\n",
      "723310 723310\n",
      "808315 808315\n",
      "268538 268538\n",
      "214738 214738\n",
      "513754 513754\n",
      "238848 238848\n",
      "560699 560699\n",
      "710333 710333\n",
      "364896 364896\n",
      "258303 258303\n",
      "150926 150926\n",
      "433369 433369\n",
      "279269 279269\n",
      "149001 149001\n",
      "893652 893652\n",
      "290641 290641\n",
      "708611 708611\n",
      "519458 519458\n",
      "301886 301886\n",
      "723433 723433\n",
      "300375 300375\n",
      "142514 142514\n",
      "221052 221052\n",
      "236142 236142\n",
      "225762 225762\n",
      "256055 256055\n",
      "554561 554561\n",
      "186296 186296\n",
      "606168 606168\n",
      "745637 745637\n",
      "358051 358051\n",
      "623925 623925\n",
      "340112 340112\n",
      "544354 544354\n",
      "592107 592107\n",
      "839371 839371\n",
      "480149 480149\n",
      "462147 462147\n",
      "545821 545821\n",
      "490622 490622\n",
      "636270 636270\n",
      "679694 679694\n",
      "652343 652343\n",
      "330303 330303\n",
      "863069 863069\n",
      "498131 498131\n",
      "103624 103624\n",
      "337125 337125\n",
      "237411 237411\n",
      "852592 852592\n",
      "875249 875249\n",
      "899097 899097\n",
      "406160 406160\n",
      "776357 776357\n",
      "437267 437267\n",
      "272629 272629\n",
      "209271 209271\n",
      "728979 728979\n",
      "184798 184798\n",
      "708458 708458\n",
      "497763 497763\n",
      "648620 648620\n",
      "160479 160479\n",
      "107749 107749\n",
      "863824 863824\n",
      "777620 777620\n",
      "173960 173960\n",
      "259255 259255\n",
      "498722 498722\n",
      "353129 353129\n",
      "669865 669865\n",
      "785003 785003\n",
      "885911 885911\n",
      "304822 304822\n",
      "143222 143222\n",
      "685388 685388\n",
      "791156 791156\n",
      "585369 585369\n",
      "688782 688782\n",
      "773025 773025\n",
      "474541 474541\n",
      "615736 615736\n",
      "872262 872262\n",
      "247352 247352\n",
      "304856 304856\n",
      "254052 254052\n",
      "378142 378142\n",
      "824016 824016\n",
      "529313 529313\n",
      "670130 670130\n",
      "761422 761422\n",
      "296350 296350\n",
      "280636 280636\n",
      "424345 424345\n",
      "533262 533262\n",
      "258333 258333\n",
      "181736 181736\n",
      "553081 553081\n",
      "358595 358595\n",
      "396017 396017\n",
      "621155 621155\n",
      "436050 436050\n",
      "371445 371445\n",
      "417391 417391\n",
      "290782 290782\n",
      "168413 168413\n",
      "682346 682346\n",
      "386648 386648\n",
      "348308 348308\n",
      "239878 239878\n",
      "800099 800099\n",
      "184776 184776\n",
      "678379 678379\n",
      "757742 757742\n",
      "851304 851304\n",
      "267614 267614\n",
      "266282 266282\n",
      "317592 317592\n",
      "696759 696759\n",
      "122762 122762\n",
      "805377 805377\n",
      "528656 528656\n",
      "490467 490467\n",
      "318561 318561\n",
      "324033 324033\n",
      "237209 237209\n",
      "797240 797240\n",
      "368486 368486\n",
      "566113 566113\n",
      "633432 633432\n",
      "143481 143481\n",
      "551599 551599\n",
      "254292 254292\n",
      "214837 214837\n",
      "601129 601129\n",
      "460230 460230\n",
      "762286 762286\n",
      "366768 366768\n",
      "888965 888965\n",
      "754737 754737\n",
      "517256 517256\n",
      "721196 721092\n",
      "631422 631422\n",
      "832481 832481\n",
      "714564 714564\n",
      "581097 581097\n",
      "815826 815826\n",
      "528112 528112\n",
      "103309 103309\n",
      "442946 442946\n",
      "378796 378796\n",
      "856027 856027\n",
      "176552 176552\n",
      "839462 839462\n",
      "152424 152424\n",
      "108188 108188\n",
      "267362 267362\n",
      "693146 693146\n",
      "180062 180062\n",
      "803929 803929\n",
      "668138 668138\n",
      "667983 667983\n",
      "128432 128432\n",
      "378333 378333\n",
      "397643 397643\n",
      "448105 448105\n",
      "594383 594383\n",
      "891553 891553\n",
      "565501 565501\n",
      "681799 681799\n",
      "610566 610566\n",
      "161362 161362\n",
      "518649 518649\n",
      "322909 322909\n",
      "597669 597669\n",
      "590452 590452\n",
      "203820 203820\n",
      "301235 301235\n",
      "456331 456331\n",
      "180941 180941\n",
      "860497 860497\n",
      "695428 695428\n",
      "622956 622956\n",
      "435230 435230\n",
      "704597 704597\n",
      "178791 178791\n",
      "452189 452189\n",
      "527784 527784\n",
      "739652 739652\n",
      "242124 242124\n",
      "288055 288055\n",
      "428238 428238\n",
      "706232 706232\n",
      "200523 200523\n",
      "514234 514234\n",
      "230877 230877\n",
      "259650 259650\n",
      "385682 385682\n",
      "686750 686750\n",
      "476180 476180\n",
      "383176 383176\n",
      "224390 224390\n",
      "336754 336754\n",
      "701901 701901\n",
      "633125 633125\n",
      "452647 452647\n",
      "279262 279262\n",
      "197405 197405\n",
      "360803 360903\n",
      "451299 451299\n",
      "780493 780493\n",
      "775399 775399\n",
      "412664 412664\n",
      "787290 787290\n",
      "516748 516748\n",
      "775351 775351\n",
      "203240 203240\n",
      "312711 312711\n",
      "544138 544138\n",
      "578613 578613\n",
      "104420 104420\n",
      "645306 645306\n",
      "689337 689337\n",
      "340449 340449\n",
      "694877 694877\n",
      "392264 392264\n",
      "605158 605158\n",
      "481870 481870\n",
      "202712 202712\n",
      "866485 866485\n",
      "582991 582991\n",
      "451872 451872\n",
      "474491 474491\n",
      "137667 137667\n",
      "112374 112374\n",
      "517661 517661\n",
      "610030 610030\n",
      "764064 764064\n",
      "291564 291564\n",
      "489863 489863\n",
      "853964 853964\n",
      "215101 215101\n",
      "871465 871465\n",
      "519885 519885\n",
      "141120 141120\n",
      "535392 535392\n",
      "815586 815586\n",
      "507677 507677\n",
      "789915 789915\n",
      "612040 612040\n",
      "862113 862113\n",
      "762433 762433\n",
      "319375 319375\n",
      "214901 214901\n",
      "288559 288559\n",
      "637511 637511\n",
      "844650 844650\n",
      "206096 206096\n",
      "218335 218335\n",
      "560316 560316\n",
      "772732 772732\n",
      "201979 201979\n",
      "493870 493870\n",
      "382489 382489\n",
      "277075 277075\n",
      "142098 142098\n",
      "152186 152186\n",
      "297355 297355\n",
      "838818 838818\n",
      "668924 668924\n",
      "595542 595542\n",
      "704734 704734\n",
      "108088 108088\n",
      "531537 531537\n",
      "384856 384856\n",
      "540063 540063\n",
      "495775 495775\n",
      "174900 174900\n",
      "413106 413106\n",
      "402342 402342\n",
      "749319 749319\n",
      "524244 524244\n",
      "783359 783359\n",
      "121092 121092\n",
      "562520 562520\n",
      "549250 549250\n",
      "280934 280934\n",
      "670733 670733\n",
      "273769 273769\n",
      "635216 635216\n",
      "750572 750572\n",
      "690385 690385\n",
      "643762 643762\n",
      "361526 361526\n",
      "614753 614753\n",
      "363007 363007\n",
      "776866 776866\n",
      "110407 110407\n",
      "522839 522839\n",
      "728134 728134\n",
      "836670 836670\n",
      "370925 370925\n",
      "631821 631821\n",
      "527220 527220\n",
      "608817 608817\n",
      "588565 588565\n",
      "158783 158783\n",
      "468165 468165\n",
      "836729 836729\n",
      "749630 749630\n",
      "486576 486576\n",
      "636948 636948\n",
      "503309 503309\n",
      "730882 730882\n",
      "844221 844221\n",
      "347452 347452\n",
      "722283 722283\n",
      "229356 229356\n",
      "195136 195136\n",
      "690557 690557\n",
      "137906 137906\n",
      "602578 602578\n",
      "561747 561747\n",
      "130817 130817\n",
      "506306 506306\n",
      "356596 356596\n",
      "793901 793901\n",
      "141528 141528\n",
      "838774 838774\n",
      "228921 228921\n",
      "702675 702675\n",
      "520985 520985\n",
      "702850 702850\n",
      "440376 440376\n",
      "546573 546573\n",
      "572832 572832\n",
      "810745 810745\n",
      "159105 159105\n",
      "845347 845347\n",
      "694750 694750\n",
      "464237 464237\n",
      "735887 735887\n",
      "837451 837451\n",
      "880288 880288\n",
      "102763 102763\n",
      "787162 787162\n",
      "292539 292539\n",
      "394457 394457\n",
      "579744 579744\n",
      "393378 393378\n",
      "752793 752793\n",
      "181358 181358\n",
      "144518 144518\n",
      "664692 664692\n",
      "529561 529561\n",
      "672558 672558\n",
      "708125 708125\n",
      "176044 176044\n",
      "522467 522467\n",
      "618333 618333\n",
      "452220 452220\n",
      "509629 509629\n",
      "225318 225318\n",
      "295911 295911\n",
      "256199 256199\n",
      "892639 892639\n",
      "277996 277996\n",
      "370156 370156\n",
      "217697 217697\n",
      "197686 197686\n",
      "221363 221363\n",
      "787804 787804\n",
      "506515 506515\n",
      "540568 540568\n",
      "100116 100116\n",
      "534818 534818\n",
      "245173 245173\n",
      "385210 385210\n",
      "327590 327590\n",
      "534512 534512\n",
      "134639 134639\n",
      "523466 523466\n",
      "280050 280050\n",
      "178645 178645\n",
      "642363 642363\n",
      "251105 251105\n",
      "110260 110260\n",
      "207856 207856\n",
      "295080 295080\n",
      "773496 773496\n",
      "107431 107431\n",
      "185685 185685\n",
      "771859 771859\n",
      "537879 537879\n",
      "319718 319718\n",
      "863766 863766\n",
      "573877 573877\n",
      "755460 755460\n",
      "281332 281332\n",
      "124586 124586\n",
      "698088 698088\n",
      "388841 388841\n",
      "276211 276211\n",
      "307524 307524\n",
      "392707 392707\n",
      "388282 388282\n",
      "520915 520915\n",
      "648453 648453\n",
      "213910 213910\n",
      "198082 198082\n",
      "137876 137876\n",
      "680007 680007\n",
      "771295 771295\n",
      "475598 475598\n",
      "827921 827921\n",
      "261190 261190\n",
      "541910 541910\n",
      "314752 314752\n",
      "643494 643494\n",
      "393555 393555\n",
      "245023 245023\n",
      "778157 778157\n",
      "607633 607633\n",
      "501066 501066\n",
      "322404 322404\n",
      "471528 471528\n",
      "507074 507074\n",
      "183665 183665\n",
      "298234 298234\n",
      "168264 168264\n",
      "211255 211255\n",
      "114150 114150\n",
      "493102 493102\n",
      "686491 686491\n",
      "734202 734202\n",
      "421793 421793\n",
      "427813 427813\n",
      "321223 321223\n",
      "358183 358183\n",
      "431726 431726\n",
      "737697 737697\n",
      "875461 875461\n",
      "407569 407569\n",
      "869401 869401\n",
      "126810 126810\n",
      "550271 550271\n",
      "663910 663910\n",
      "450073 450073\n",
      "219509 219509\n",
      "676288 676288\n",
      "839885 839885\n",
      "375057 375057\n",
      "725041 725041\n",
      "306251 306251\n",
      "457309 457309\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"\\nloss    : {}\\naccuracy: {}\".format(results[0], results[1] * 100))\n",
    "\n",
    "test_amount = 1000\n",
    "output = model.predict(x_test)[:test_amount]\n",
    "pred_digits = map(one_hot_decode_digits_to_str, output)\n",
    "truth_digits = map(one_hot_decode_digits_to_str, y_test[:test_amount])\n",
    "\n",
    "for pred, truth in zip(pred_digits, truth_digits):\n",
    "    print(pred, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
